{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b3ae4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b513470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshvive14/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harshvive14/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "SEED = 0\n",
    "Features_count = 6\n",
    "SW = stopwords.words(\"english\")\n",
    "PUNCT = list(punctuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "337c14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createData(data):\n",
    "    words = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    for d in tqdm(data):\n",
    "        tags = d[\"ner_tags\"]\n",
    "        tokens = d[\"tokens\"]\n",
    "        for i in range(len(tokens)):\n",
    "            x = vectorize(w = tokens[i], scaled_position = (i/len(tokens)))\n",
    "            if tags[i] <= 0:\n",
    "                y = 0\n",
    "            else:\n",
    "                y = 1\n",
    "            features.append(x)\n",
    "            labels.append(y)\n",
    "        words += tokens\n",
    "    words = np.asarray(words, dtype = \"object\")\n",
    "    features = np.asarray(features, dtype = np.float32)\n",
    "    labels = np.asarray(labels, dtype = np.float32)\n",
    "    return words, features, labels\n",
    "\n",
    "def vectorize(w, scaled_position):\n",
    "    v = np.zeros(Features_count).astype(np.float32)\n",
    "    title = 0\n",
    "    allcaps = 0\n",
    "    sw = 0\n",
    "    punct = 0\n",
    "    # If first character in uppercase\n",
    "    if w[0].isupper():\n",
    "        title = 1\n",
    "    # All characters in uppercase\n",
    "    if w.isupper():\n",
    "        allcaps = 1\n",
    "    # Is stopword\n",
    "    if w.lower() in SW:\n",
    "        sw = 1\n",
    "    # Is punctuation\n",
    "    if w in PUNCT:\n",
    "        punct = 1\n",
    "    return [title, allcaps, len(w), sw, punct, scaled_position]\n",
    "\n",
    "\n",
    "def infer(model, scaler, s): # To perform inference\n",
    "    tokens = word_tokenize(s)\n",
    "    features = []\n",
    "    l = len(tokens)\n",
    "    for i in range(l):\n",
    "        f = vectorize(w = tokens[i], scaled_position = (i/l))\n",
    "        features.append(f)\n",
    "    features = np.asarray(features, dtype = np.float32)\n",
    "    scaled = scaler.transform(features)\n",
    "    pred = model.predict(scaled)\n",
    "    return pred, tokens, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "921d490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/Users/harshvive14/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4041fdcccdfb4683bbee3238897db650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103a9cbb6174c7481ffacba62959195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619a7e8b04ce48c689ce6c0cccf0c673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e26495bf79a464f94daabef55ec96f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]..................\n",
      "Warning: using -h 0 may be faster\n",
      "*.........\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 27236\n",
      "obj = -15443.285139, rho = -1.088682\n",
      "nSV = 17088, nBSV = 9558\n",
      "Total nSV = 17088\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.96      0.98     42759\n",
      "         1.0       0.82      0.97      0.89      8603\n",
      "\n",
      "    accuracy                           0.96     51362\n",
      "   macro avg       0.91      0.96      0.93     51362\n",
      "weighted avg       0.96      0.96      0.96     51362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"conll2003\")\n",
    "\n",
    "# <'id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'> \n",
    "\n",
    "data_train = data[\"train\"] \n",
    "data_val   = data[\"validation\"]\n",
    "data_test  = data[\"test\"]\n",
    "\n",
    "words_train, X_train, y_train = createData(data_train)\n",
    "words_val, X_val, y_val       = createData(data_val)\n",
    "words_test, X_test, y_test    = createData(data_test)\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "model = SVC(C = 1.0, kernel = \"linear\", class_weight = \"balanced\", random_state = SEED, verbose = True)\n",
    "\n",
    "# C : Regularization parameter.\n",
    "# Verbose: To takes advantage of a per-process runtime setting in libsvm.\n",
    "\n",
    "model.fit(X_train, y_train) # 'MODEL-TRAINING'\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "nei_model_name = 'nei_model.sav'\n",
    "pickle.dump(model, open(nei_model_name, 'wb'))\n",
    "\n",
    "scaler_model_name = 'scaler_model.sav'\n",
    "pickle.dump(scaler, open(scaler_model_name, 'wb'))\n",
    "\n",
    "print(classification_report(y_true = y_val, y_pred = y_pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e5dd3a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m st\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamed-Entity Identification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mtext_input(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter input string here: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m st\u001b[38;5;241m.\u001b[39mbutton(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess Text\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "st.title(\"Named-Entity Identification\")\n",
    "input = st.text_input(\"Enter input string here: \")\n",
    "if st.button(\"Process Text\"):\n",
    "    pred, tokens, features = infer(model, scaler, input)\n",
    "    annotated = []\n",
    "    for w, p in zip(tokens, pred):\n",
    "        annotated.append(f\"{w}_{int(p)}\")\n",
    "    output = \" \".join(annotated)\n",
    "    st.write(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "249d5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Harsh Vivek is smart and awesome\"\n",
    "nei_model = pickle.load(open(\"nei_model.sav\", 'rb'))\n",
    "scaler_model = pickle.load(open(\"scaler_model.sav\", 'rb'))\n",
    "\n",
    "pred, tokens, features = infer(nei_model, scaler_model, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "434e9504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 0., 0., 0., 0.], dtype=float32),\n",
       " ['Harsh', 'Vivek', 'is', 'smart', 'and', 'awesome'],\n",
       " array([[1.        , 0.        , 5.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [1.        , 0.        , 5.        , 0.        , 0.        ,\n",
       "         0.16666667],\n",
       "        [0.        , 0.        , 2.        , 1.        , 0.        ,\n",
       "         0.33333334],\n",
       "        [0.        , 0.        , 5.        , 0.        , 0.        ,\n",
       "         0.5       ],\n",
       "        [0.        , 0.        , 3.        , 1.        , 0.        ,\n",
       "         0.6666667 ],\n",
       "        [0.        , 0.        , 7.        , 0.        , 0.        ,\n",
       "         0.8333333 ]], dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, tokens, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "605591d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = []\n",
    "for w, p in zip(tokens, pred):\n",
    "    annotated.append(f\"{w}_{int(p)}\")\n",
    "output = \" \".join(annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46077c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harsh_1 Vivek_1 is_0 smart_0 and_0 awesome_0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2d786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789343b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
